{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-5G7L_SQW2S"
   },
   "source": [
    "# **Project: Apriori Algorithm for Finding Frequent Itemsets with PySpark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhGFNpnRip7Y"
   },
   "source": [
    "## Task 1: Import the Libraries and Set Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "udCL8NmOmA5_"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "tdFQEojVmEjz",
    "outputId": "154b9dba-9995-4d35-9a81-4af922b191a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/02 15:37:57 WARN Utils: Your hostname, Hannahs-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.118 instead (on interface en0)\n",
      "23/12/02 15:37:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/02 15:37:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.118:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rivana</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa840adb2b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize a Spark session\n",
    "conf=pyspark.SparkConf().setAppName('rivana').setMaster('local')\n",
    "sc=pyspark.SparkContext(conf=conf)\n",
    "spark=SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZaI7bSMiyt0"
   },
   "source": [
    "# Task 2: Generate Combinations—Parent Intersection Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For combinations of length k,k−2 elements must be common in both parents. \\\n",
    "All subsets of the itemset must be frequent. \\\n",
    "The itemset must have a count greater than the specified support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freq_k_1: A list of frequent itemsets proposed with length k−1 \\\n",
    "k: The size of the itemsets to create. \\\n",
    "k_size_comb: A list containing combinations of size k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function generates candidate itemsets of size k based on the frequent itemsets of size (k-1) discovered in the previous iteration of the Apriori algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OvzFHvnd5Jlh"
   },
   "outputs": [],
   "source": [
    "def pre_check(freq_k_1,k):\n",
    "    k_size_comb=[]\n",
    "    for i in range(len(freq_k_1)):\n",
    "        x=set(freq_k_1[i])\n",
    "        for j in range(len(freq_k_1)):\n",
    "            y=set(freq_k_1[j])\n",
    "   #Ensures that the combination is considered only once by checking that j is less than i.\n",
    "            if j<i:\n",
    "                if len(x.intersection(y))>=(k-2):\n",
    "                    k_size_comb.append(tuple(sorted(list(x.union(y)))))\n",
    "    return k_size_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OwdhO5r6FrX"
   },
   "source": [
    "## Task 3: Generate Combinations—Subset Frequency Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the second rule of the Apriori algorithm, a combination is frequent only if all its subsets are frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns the list of combinations that passed the filtering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "vUIK4w2p5_xq"
   },
   "outputs": [],
   "source": [
    "import itertools  # Import the itertools module\n",
    "\n",
    "def post_check(k_size_comb, freq_k_1, k):\n",
    "    filtered = []  # Initialize an empty list to store filtered combinations\n",
    "    for comb in k_size_comb:\n",
    "        flag = False  # Initialize a flag to track whether all (k-1)-subsets are in freq_k_1\n",
    "        for sub_comb in itertools.combinations(comb, k - 1):\n",
    "            if sub_comb not in freq_k_1:\n",
    "                flag = True  # Set the flag to True if any (k-1)-subset is not in freq_k_1\n",
    "        if flag == False:\n",
    "            filtered.append(tuple(comb))  # Append the combination to the filtered list if all (k-1)-subsets are in freq_k_1\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExJPl1hE6rNn"
   },
   "source": [
    "## Task 4: Count Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function counts the occurrences of combinations in a list of lines and filters those combinations based on a specified support count (supCount). The result is a list of combinations that satisfy the support count condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Filtered Combinations:filtered_combinations is a list of combinations obtained from a previous step (e.g., post_check). \n",
    "\n",
    "2)Lines:lines is a list of transactions where each transaction is represented as a list of items. \n",
    "\n",
    "3)Support Count Threshold:supCount is set to 2, meaning we want to filter combinations that occur in at least 2 transactions. \n",
    "\n",
    "Function Call: count_check(filtered_combinations, lines, supCount): The function is called with the filtered combinations, the list of lines, and the support count threshold. \n",
    "\n",
    "Execution:\n",
    "\n",
    "The function iterates over each combination in filtered_combinations.\n",
    "For each combination, it checks the presence of each item in the combination in the list of lines.\n",
    "If all items in the combination are present in a line, the count for that combination is incremented.\n",
    "After processing all combinations, the function filters combinations based on the support count threshold. \n",
    "\n",
    "Result:The result is a list of combinations that meet the support count criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FIkCvqPd6rqA"
   },
   "outputs": [],
   "source": [
    "def count_check(filtered, lines, supCount):\n",
    "    results = []  # Initialize an empty list to store combinations that meet the support count criteria\n",
    "    counts = dict(zip(filtered, [0]*len(filtered)))  # Initialize a dictionary to store counts for each combination\n",
    "\n",
    "    # Iterate over each combination in the filtered list\n",
    "    for combination in filtered:\n",
    "        present = [False]*len(combination)  # Initialize a list to track the presence of each item in the combination\n",
    "        for i in range(len(combination)):\n",
    "            # Iterate over each line in the list of lines\n",
    "            for line in lines:\n",
    "                if combination[i] in line:\n",
    "                    present[i] = True  # Set the flag to True if the item is present in the line\n",
    "                if all(present):\n",
    "                    counts[combination] += 1  # Increment the count for the combination if all items are present in a line\n",
    "\n",
    "    # Iterate over the counts dictionary and filter combinations based on the support count\n",
    "    for word, count in counts.items():\n",
    "        if count >= supCount:\n",
    "            results.append(word)  # Append combinations with counts greater than or equal to supCount to the results list\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'B', 'C'), ('B', 'C', 'D'), ('A', 'B', 'D')]\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "filtered_combinations = [\n",
    "    ('A', 'B', 'C'),\n",
    "    ('B', 'C', 'D'),\n",
    "    ('A', 'B', 'D'),\n",
    "]\n",
    "\n",
    "lines = [\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "    ['B', 'C', 'E'],\n",
    "    ['A', 'B', 'D', 'F'],\n",
    "    ['A', 'B', 'C', 'D'],\n",
    "]\n",
    "\n",
    "supCount = 2  # Support count threshold\n",
    "\n",
    "# Call the count_check function\n",
    "result = count_check(filtered_combinations, lines, supCount)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA9E_8cJ8dI0"
   },
   "source": [
    "## Task 5: Generate k-Size Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "N3nGr4MSspgf"
   },
   "outputs": [],
   "source": [
    "def generator(freq_k_1, k, partition, support):\n",
    "    \n",
    "    lines = list(partition)\n",
    "    supCount = len(lines)*support\n",
    "\n",
    "    k_size_comb = pre_check(freq_k_1, k)\n",
    "    \n",
    "    filtered = post_check(k_size_comb, freq_k_1, k)\n",
    "    \n",
    "    return count_check(filtered, lines, supCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpxtsLXmjBPP"
   },
   "source": [
    "## Task 6: Generate Singles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preapre the data to be fed into the generator() function bt creating a function named get_singles().\n",
    "This function will take the dataset partition available to the worked node as input and return the frequent words observed in it as tuples of size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "SMxaQyu0FZXg"
   },
   "outputs": [],
   "source": [
    "def get_singles(lines, support):\n",
    "    supCount = len(list(lines))*support\n",
    "    vocab = set([])\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            vocab.add(word)\n",
    "    counts = dict(zip(vocab, [0]*len(list(vocab))))\n",
    "    combinations = []\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            counts[word] +=1\n",
    "    for word, count in counts.items():\n",
    "        if (count>=supCount):\n",
    "            combinations.append(tuple((word,))) \n",
    "    return sorted(combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fepmtz7xjVMH"
   },
   "source": [
    "## Task 7: The Worker Partition Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map partitions at the worker nodes. \\\n",
    "Take the partition at the worker nodes and generate the sequences that have an occurrence count greater than the support count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the maximum length of sequences to generate seq_len, \n",
    "#and the apriori() function that implement the Apriori algorithm\n",
    "seq_len = sc.broadcast(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "9DdX671FmKjA"
   },
   "outputs": [],
   "source": [
    "def apriori(iterator):\n",
    "    partition = []\n",
    "    for v in iterator:\n",
    "        partition.append(v)\n",
    "    support = sup.value\n",
    "    results= get_singles(partition, support)\n",
    "    print('starting with', results)\n",
    "\n",
    "    for k in range(2, seq_len.value+1):\n",
    "        print('sequence length', k)\n",
    "     \n",
    "        combos = generator(results, k, partition, support)\n",
    "\n",
    "        if len(combos) == 0:\n",
    "            print('ending at sequence length' ,k-1)\n",
    "            return results\n",
    "\n",
    "        results = combos\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FU7iixEgkWbv"
   },
   "source": [
    "## Task 8: Load Data and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5nD2F9BPphx",
    "outputId": "749f9619-c2da-414b-e026-b40ccbac256b"
   },
   "source": [
    "Load the data to a RDD (resilent distributed dataset) and perform preprocessing jobs \n",
    "* RDDs enable parallel distributed processing and are immutable elements; you can't alter them after creation. \n",
    "* When you call an operation on an RDD, PySpark performs lazy evaluations;\n",
    "operations are not immediately performed and are onlt executed when an action such as collect is invoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.textFile(\"/Users/hannah/Documents/projects/Dataset.csv\")\n",
    "tagsheader = rdd.first() \n",
    "tags = sc.parallelize(tagsheader)\n",
    "seq_len = sc.broadcast(3)\n",
    "data = rdd.subtract(tags)\n",
    "length = sc.broadcast(data.count())\n",
    "sup = sc.broadcast(0.03)\n",
    "lines = data.map(lambda x: x.lstrip('\"').rstrip('\"').split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jchi9CtAko95"
   },
   "source": [
    "## Task 9: The Distributed Transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the apriori() function to retrieve the frequenct combinations for each partition in a parallel distributed manner\n",
    "* rdd.mapPartitions(): mine combinations in a parallel manner. The method accepts the name of the function you want to map the partitions with as aparameter.\n",
    "* rdd.distinct(): drop duplicates in the RDD\n",
    "* rdd.collect() action to collect the data at the master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--FRHYLAkomc",
    "outputId": "24b88d56-44c4-470b-fbee-11fc696cd468",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "starting with [('avocado',), ('burgers',), ('butter',), ('cake',), ('cereals',), ('champagne',), ('chicken',), ('chocolate',), ('cookies',), ('cooking oil',), ('eggs',), ('escalope',), ('french fries',), ('fresh bread',), ('frozen smoothie',), ('frozen vegetables',), ('grated cheese',), ('green tea',), ('ground beef',), ('herb & pepper',), ('honey',), ('light mayo',), ('low fat yogurt',), ('milk',), ('mineral water',), ('olive oil',), ('pancakes',), ('red wine',), ('salmon',), ('shrimp',), ('soup',), ('spaghetti',), ('tomato juice',), ('tomatoes',), ('turkey',), ('whole wheat pasta',), ('whole wheat rice',)]\n",
      "sequence length 2\n",
      "sequence length 3\n",
      "starting with [('avocado',), ('brownies',), ('burgers',), ('cake',), ('champagne',), ('chicken',), ('chocolate',), ('cookies',), ('cooking oil',), ('eggs',), ('energy bar',), ('escalope',), ('french fries',), ('fresh bread',), ('frozen smoothie',), ('frozen vegetables',), ('grated cheese',), ('green tea',), ('ground beef',), ('herb & pepper',), ('honey',), ('hot dogs',), ('low fat yogurt',), ('milk',), ('mineral water',), ('oil',), ('olive oil',), ('pancakes',), ('pepper',), ('salmon',), ('shrimp',), ('soup',), ('spaghetti',), ('tomato juice',), ('tomatoes',), ('turkey',), ('whole wheat rice',)]\n",
      "sequence length 2\n",
      "sequence length 3======================>                            (1 + 1) / 2]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "freq=lines.mapPartitions(apriori)\n",
    "freq=freq.distinct()\n",
    "comb=freq.collect()\n",
    "# print(\"Possible frequent itemset(s):\\n\", comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7u2Ysdbj3CO"
   },
   "source": [
    "## Task 10: Auxiliary Function to Check Presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the worker nodes have computed the list of frequent combinations, the master node should check their frequency centrally.\n",
    "* create an auxiliary() function that takes in the distinct combinations and a dataset row, and yields the combinations present in the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "5Y2193xZSQzh"
   },
   "outputs": [],
   "source": [
    "def auxiliary(row, combinations):\n",
    "    present= []\n",
    "    for combination in combinations:\n",
    "        presence = [False]*len(combination)\n",
    "        for i in range(len(combination)):\n",
    "            presence[i] = combination[i] in row\n",
    "        if all(presence):\n",
    "            present+=[combination]\n",
    "    return present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XZIIpNak8aK"
   },
   "source": [
    "## Task 11: Count Check at Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the auxiliary function to determine the occurrence counts of the combinations and filter them out if it is less than the support.\n",
    "* Broadcast the combinations collected from the apriori() method.\n",
    "* Generate tuples with the (combinations,1) format ever time a combination occurs in a dataset.\n",
    "* Use the RDD.flatMap() method to convert a list of lists into a single list.\n",
    "* Use the Rdd.reduceBy Key() method to generate occurrence counts of each combination.\n",
    "* Filter out the combinations that have occurrence counts less than the support count.\n",
    "* Collect the filtered combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you pause too long between running apriori() and running the following code block, you will get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('eggs', 'ground beef', 'spaghetti')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb = sc.broadcast(comb)\n",
    "freq1 = lines.map(lambda x: [(key, 1) for key in auxiliary(x, comb.value)]).filter(lambda x: len(x)>0)\n",
    "\n",
    "freq2 = freq1.flatMap(lambda x: x)\n",
    "freq3 = freq2.reduceByKey(lambda x, y: x+y)\n",
    "freq4 = freq3.filter(lambda x: x[1]>sup.value*length.value).map(lambda x: x[0])\n",
    "freq4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzVqAezSRaqr",
    "outputId": "94eda06b-27a3-4331-e134-664d22b7d72f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As9mbaSplI7V"
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
